{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for English stimuli\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<span style=\"color:red\">Before you run the demo, make sure to follow the steps from the README.md file.</span>\n",
    "\n",
    "<span style=\"color:red\">If you want to learn more about the underlying implementation, use the help command.</span>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary libraries, and (optionally) set the cache folder for the context-dependent models (i.e., Hugging Face transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['TRANSFORMERS_CACHE'] = <new_cache_folder_path>\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sources.final_model import ConcretextRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ridge regression model (i.e., ConcretextRegressor). We enforce a strong degree of regularization (i.e., lambda_param=500), and run the model in verbose mode (i.e., verbose=True), since this allows us to detect potential bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model = ConcretextRegressor(lambda_param=500, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process all the behavioural norms and distributional models. Alternatively, you can process only a subset of norms have low predictive power and/or take too much time to load. You must assign a name to each norm/model, and it is this name that you will later use if you wish to generate predictors based on that particular norm/model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the behavioural norms from file. If you do not plan to use norms at all, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_norm_names = ['Conc', 'SemD', 'Freq_CD', 'AoA', 'Emo', 'SensMot']\n",
    "behav_norm_filenames = ['Concreteness norms - English.txt', \n",
    "                       'Semantic diversity norms - English.txt',\n",
    "                       'Frequency and contextual diversity norms - English.txt',\n",
    "                       'Age of acquisition norms - English.txt',\n",
    "                       'Emotional norms - English.txt',\n",
    "                       'Sensorimotor norms - English.txt']\n",
    "\n",
    "curr_model.load_behav_norms(behav_norm_names, behav_norm_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the context-independent models from file, then reduce the dimensionality of the models and (optionally) their concatenation. Alternatively, you can decide to omit the dimensionality reduction step, or reduce the dimensionality of only a subset of models. If you do not plan to use context-independent models at all, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_indep_model_names = ['Skip-gram', 'GloVe', 'NumberBatch']\n",
    "cont_indep_model_filenames = ['Skip-gram embeddings - English.txt',\n",
    "                              'GloVe embeddings - English.txt',\n",
    "                              'ConceptNet NumberBatch embeddings - English.txt']\n",
    "\n",
    "curr_model.load_cont_indep_models(cont_indep_model_names, cont_indep_model_filenames, include_concat=True)\n",
    "\n",
    "curr_model.reduce_dims_cont_indep_models(cont_indep_model_names, include_concat=True, n_pcs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the (pre-trained) context-dependent models, using the Hugging Face library. For English, the classes of models (i.e., transformers) currently supported by our implementation are 'albert', 'bart', 'bert', 'gpt-2', and 'roberta'. Each class has one or more available models (e.g., in the case of BERT, valid ids are 'bert-base-uncased', 'bert-base-cased', 'bert-large-cased', etc.; you can find the full list at https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_dep_model_names = ['albert', 'bart', 'bert', 'gpt-2']\n",
    "cont_dep_model_ids = ['albert-base-v2', 'facebook/bart-base', 'bert-base-uncased', 'gpt2']\n",
    "\n",
    "curr_model.load_cont_dep_models(cont_dep_model_names, cont_dep_model_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previously loaded norms/models and their corresponding names, select one or more types of predictors that will be used in fitting the concreteness ratings. For each selected norm/model, you need to specify whether the predictors should be derived from the inflected form of the target (i.e., the word from position INDEX in TEXT), and/or the uninflected one (i.e., the word from TARGET). Also, for any given norm/model, you can include both the inflected and uninflected versions of the predictors (e.g., by setting pred_names = ['MyNormOrModel', 'MyNormOrModel'], target_is_inflected = [True, False])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_names = ['Conc', 'SemD', 'Freq_CD', 'AoA', 'Emo', 'SensMot', 'albert', 'bart', 'bert', 'gpt-2']\n",
    "target_is_inflected = [True, True, True, True, True, True, True, True, True, True]\n",
    "\n",
    "curr_model.select_preds_original(pred_names, target_is_inflected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the (trial) stimuli from file. Like in the case of the norms and models, you are free to provide your own set of stimuli, as long as they follow the format employed by the organizers of CONcreTEXT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = pd.read_csv('./stimuli/CONcreTEXT_trial_EN.tsv', sep='\\t')\n",
    "\n",
    "stimuli_y = stimuli['MEAN'];\n",
    "stimuli_X = stimuli.drop(['MEAN'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, test the selected predictors through 5-fold cross-validation. Model performance is measured via the Pearson and Spearman correlations.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "\n",
    "kf = KFold(n_splits, shuffle=True)\n",
    "\n",
    "res_pearson = []\n",
    "res_spearman = []\n",
    "\n",
    "for train_index, test_index in kf.split(stimuli_X):\n",
    "    \n",
    "    X_train, X_test = stimuli_X.iloc[train_index,:], stimuli_X.iloc[test_index,:]\n",
    "    y_train, y_test = stimuli_y[train_index], stimuli_y[test_index]\n",
    "    \n",
    "    pred_list = curr_model.fit(X_train, y_train)  \n",
    "    pearson_corr, spearman_corr = curr_model.score(X_test, y_test)\n",
    "    \n",
    "    print('Pearson correlation: {:.2f}'.format(pearson_corr))\n",
    "    print('Spearman correlation: {:.2f}'.format(spearman_corr))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    res_pearson.append(pearson_corr)\n",
    "    res_spearman.append(spearman_corr) \n",
    "        \n",
    "print('Mean correlation (Pearson): {:.2f}'.format(np.mean(res_pearson)))\n",
    "print('Mean correlation (Spearman): {:.2f}'.format(np.mean(res_spearman)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
