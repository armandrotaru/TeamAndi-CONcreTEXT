{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for Italian stimuli\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<span style=\"color:red\">Before you run the demo, make sure to follow the steps from the README.md file.</span>\n",
    "\n",
    "<span style=\"color:red\">If you want to learn more about the underlying implementation, use the help command.</span>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary libraries, and (optionally) set the cache folder for the context-dependent models (i.e., Hugging Face transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['TRANSFORMERS_CACHE'] = <new_cache_folder_path>\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sources.final_model import ConcretextRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ridge regression model (i.e., ConcretextRegressor). Here we have two important options, namely to use only the Italian stimuli (i.e., include_translation=False), or both the Italian stimuli and their English translation (i.e., include_translation=True). Also, we enforce a strong degree of regularization (i.e., lambda_param=500), and run the model in verbose mode (i.e., verbose=True), since this allows us to detect potential bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model = ConcretextRegressor(include_translation=True, lambda_param=500, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process all the behavioural norms and distributional models. Alternatively, you can process only a subset of norms have low predictive power and/or take too much time to load. You must assign a name to each norm/model, and it is this name that you will later use if you wish to generate predictors based on that particular norm/model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the behavioural norms from file. If you do not plan to use norms at all, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_norm_names = ['Conc', 'SemD', 'Freq_CD', 'AoA', 'Emo', 'SensMot']\n",
    "behav_norm_filenames = ['Concreteness norms - English.txt', \n",
    "                       'Semantic diversity norms - English.txt',\n",
    "                       'Frequency and contextual diversity norms - English.txt',\n",
    "                       'Age of acquisition norms - English.txt',\n",
    "                       'Emotional norms - English.txt',\n",
    "                       'Sensorimotor norms - English.txt']\n",
    "\n",
    "curr_model.load_behav_norms(behav_norm_names, behav_norm_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the context-independent models from file, then reduce the dimensionality of the models and (optionally) their concatenation. Alternatively, you can decide to omit the dimensionality reduction step, or reduce the dimensionality of only a subset of models. If you do not plan to use context-independent models at all, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model.load_cont_indep_models(['FastText', 'NumberBatch'], \n",
    "                                  ['FastText embeddings - Italian.txt',\n",
    "                                  'ConceptNet NumberBatch embeddings - Italian.txt'],\n",
    "                                  include_concat=True)\n",
    "\n",
    "curr_model.reduce_dims_cont_indep_models(['FastText', 'NumberBatch'], include_concat=True, n_pcs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the (pre-trained) context-dependent models, using the Hugging Face library. The classes of models (i.e., transformers) currently supported by our implementation are 'albert' (English), 'alberto' (Italian'), 'bart' (English), 'bert' (English, multilingual), 'gpt-2' (English), and 'roberta' (English). Each class has one or more available models (e.g., in the case of BERT, valid ids are 'bert-base-uncased', 'bert-base-cased', 'bert-large-cased', 'bert-base-multilingual-uncased', etc.; you can find the full list at https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model.load_cont_dep_models(['albert', 'alberto', 'bart', 'bert', 'gpt-2', 'roberta'], \n",
    "                                ['albert-base-v2', \n",
    "                                 'm-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alberto',\n",
    "                                 'facebook/bart-base', \n",
    "                                 'bert-base-uncased', \n",
    "                                 'gpt2',\n",
    "                                 'roberta-base']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previously loaded norms/models and their corresponding names, select one or more types of predictors that will be used in fitting the concreteness ratings. For each selected norm/model, you need to specify whether the predictors should be derived from the inflected form of the target (i.e., the word from position INDEX in TEXT), and/or the uninflected one (i.e., the word from TARGET). Also, for any given norm/model, you can include both the inflected and uninflected versions of the predictors (e.g., by setting pred_names = ['MyNormOrModel', 'MyNormOrModel'], target_is_inflected = [True, False]).\n",
    "\n",
    "Importantly, you need to specify whether you want to generate predictors from the original Italian stimuli (i.e., by using the select_preds_original() method), or from their English translation (i.e., by using the select_preds_translated() method). You have use either both options, or a single option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model.select_preds_original(['FastText', 'NumberBatch', 'Cont_Indep_Models_Concat', 'alberto'], \n",
    "                           [False, False, False, True])\n",
    "\n",
    "curr_model.select_preds_translated(['Conc', 'Conc', 'SemD', 'SemD', 'Freq_CD', 'Freq_CD',\n",
    "                                    'AoA', 'AoA', 'Emo', 'Emo', 'SensMot', 'SensMot',\n",
    "                                    'FastText', 'albert', 'bart', 'bert', 'gpt-2', 'roberta'], \n",
    "                                   [True, False, True, False, True, False, \n",
    "                                    True, False, True, False, True, False, \n",
    "                                    True, True, True, True, True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the (trial) stimuli from file. Like in the case of the norms and models, you are free to provide your own set of stimuli, as long as they follow the format employed by the organizers of CONcreTEXT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = pd.read_csv('./stimuli/CONcreTEXT_trial_IT.tsv', sep='\\t')\n",
    "\n",
    "stimuli_y = stimuli['MEAN'];\n",
    "stimuli_X = stimuli.drop(['MEAN'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, test the selected predictors through 5-fold cross-validation. Model performance is measured via the Pearson and Spearman correlations. \n",
    "\n",
    "If you decide to use the English translation of the Italian stimuli, please keep in mind that the translation process can be rather slow, such that each fold of the cross-validation is likely to take around 5-10 minutes.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "\n",
    "kf = KFold(n_splits, shuffle=True)\n",
    "\n",
    "res_pearson = []\n",
    "res_spearman = []\n",
    "\n",
    "for train_index, test_index in kf.split(stimuli_X):\n",
    "    \n",
    "    X_train, X_test = stimuli_X.iloc[train_index,:], stimuli_X.iloc[test_index,:]\n",
    "    y_train, y_test = stimuli_y[train_index], stimuli_y[test_index]\n",
    "\n",
    "    pred_list = curr_model.fit(X_train, y_train)  \n",
    "    pearson_corr, spearman_corr = curr_model.score(X_test, y_test)\n",
    "    \n",
    "    print('Pearson correlation: {:.2f}'.format(pearson_corr))\n",
    "    print('Spearman correlation: {:.2f}'.format(spearman_corr))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    res_pearson.append(pearson_corr)\n",
    "    res_spearman.append(spearman_corr) \n",
    "        \n",
    "print('Mean correlation (Pearson): {:.2f}'.format(np.mean(res_pearson)))\n",
    "print('Mean correlation (Spearman): {:.2f}'.format(np.mean(res_spearman)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
